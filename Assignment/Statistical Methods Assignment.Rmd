---
title: "Statistical Methods"
author: "Ahmed Alhasan"
date: "10/10/2019"
output:
  html_document: default
  pdf_document: default
---


<style type="text/css">

h1.title {
  font-size: 38px;
  color: Navy;
  text-align: center;
}
h4.author { 
    font-size: 24px;
  font-family: "Times New Roman", Times, serif;
  color: Navy;
  text-align: center;
}
h4.date { 
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: Navy;
  text-align: center;
}
</style>



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br>

# 1. Computer Exercises from Course’s book

### 4.84 
Applet Exercise Refer to Exercise 4.83. Use the applet Comparison of Gamma Density Functions
to compare gamma density functions with (α = 4, β = 1), (α = 40, β = 1), and
(α = 80, β = 1).  


```{r echo=TRUE}
curve( dgamma(x,4,1), xlim=c(0,100) )
curve( dgamma(x,40,1), add=T, col='blue' )
curve( dgamma(x,80,1), add=T, col='red' )
title(main="Gamma probability distribution function")
legend(par('usr')[2], par('usr')[4], xjust=1,
       c('α=4', 'α=40', 'α=80'),
       lwd=1, lty=1,
       col=c(par('fg'), 'blue', 'red') )
```

a- What do you observe about the shapes of these three density functions? Which are less
skewed and more symmetric?  

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
The distributions goes from right skewness to symmetry with increasing α. 
</p>

<p style="color: Navy;"> 
Gamma~(4,1) is the most skewed and Gamma~(80,1) is the most symmetric since α is the highest. 
</p>
<br>

b- What differences do you observe about the location of the centers of these density functions?  

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
Centers of Gamma distributions are increasing with higher α.  
</p>
<br>

c- Give an explanation for what you observed in part (b).  

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
The means of the distributions are increasing with α.
</p>
<br>

### 4.117
Applet Exercise Use the applet Comparison of Beta Density Functions to compare beta density
functions with (α = 9, β = 7), (α = 10, β = 7), and (α = 12, β = 7).  

```{r echo=TRUE}
curve( dbeta(x,9,7), ylim=c(0,4) )
curve( dbeta(x,10,7), add=T, col='blue' )
curve( dbeta(x,12,7), add=T, col='red' )
title(main="Beta probability distribution function")
legend(par('usr')[2], par('usr')[4], xjust=1,
       c('α = 9, β = 7', 'α = 10, β = 7', 'α = 12, β = 7'),
       lwd=1, lty=1,
       col=c(par('fg'), 'blue', 'red') )
```

a- Are these densities symmetric? Skewed left? Skewed right?  

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
All are left skewed.
</p>
<br>

 
b- What do you observe as the value of α gets closer to 12?  

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
As α gets closer to 12 the Beta distribution becomes narrower and more left skewed.
</p>
<br>

c- Graph some more beta densities with α > 1, β > 1, and α > β. What do you conjecture
about the shape of beta densities with α > β and both α > 1 and β > 1? 

```{r echo=TRUE}
curve( dbeta(x,3,2), ylim=c(0,4) )
curve( dbeta(x,4,2), add=T, col='blue' )
curve( dbeta(x,5,2), add=T, col='red' )
curve( dbeta(x,7,6), add=T, col='green' )
curve( dbeta(x,15,12), add=T, col='orange' )
title(main="Beta probability distribution function")
legend(par('usr')[2], par('usr')[4], xjust=1,
       c('α = 3, β = 2', 'α = 4, β = 2', 'α = 5, β = 2', 'α = 7, β = 6', 'α = 15, β = 12'),
       lwd=1, lty=1,
       col=c(par('fg'), 'blue', 'red', 'green', 'orange') )
```

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
Shape of Beta distributions becomes narrower with higher values of α & β, and the closer β value to α value it will become more symmetrical. 
</p>

<p style="color: Navy;"> 
And they are always skewed right when α > β and α > 1 and β > 1.
</p>
<br>

### 4.118 
Applet Exercise Use the applet Comparison of Beta Density Functions to compare beta density functions with (α = .3, β = 4), (α = .3, β = 7), and (α = .3, β = 12). 

```{r echo=TRUE}
curve( dbeta(x,0.3,4), ylim=c(0,4) )
curve( dbeta(x,0.3,7), add=T, col='blue' )
curve( dbeta(x,0.3,12), add=T, col='red' )
title(main="Beta probability distribution function")
legend(par('usr')[2], par('usr')[4], xjust=1,
       c('α = 0.3, β = 4', 'α = 0.3, β = 7', 'α = 0.3, β = 12'),
       lwd=1, lty=1,
       col=c(par('fg'), 'blue', 'red') )
```

a- Are these densities symmetric? Skewed left? Skewed right? 

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
Right skewed.
</p>
<br>

b- What do you observe as the value of β gets closer to 12? 

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
The closer β value get closer to 12 and α is fixed at 0.3 the more it get right skewed, and the spread decreases as the value of β gets closer to 12.
</p>
<br>

c- Which of these beta distributions gives the highest probability of observing a value larger than 0.2? 

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
Beta~(0.3,4).
</p>
<br>

d- Graph some more beta densities with α < 1 and β > 1. What do you conjecture about the shape of beta densities with α < 1 andβ > 1?  

```{r echo=TRUE}
curve( dbeta(x,0.1,2), ylim=c(0,4) )
curve( dbeta(x,0.2,2), add=T, col='blue' )
curve( dbeta(x,0.5,2), add=T, col='red' )
curve( dbeta(x,0.5,6), add=T, col='green' )
curve( dbeta(x,0.5,12), add=T, col='orange' )
title(main="Beta probability distribution function")
legend(par('usr')[2], par('usr')[4], xjust=1,
       c('α = 0.1, β = 2', 'α = 0.2, β = 2', 'α = 0.5, β = 2', 'α = 0.5, β = 6', 'α = 0.5, β = 12'),
       lwd=1, lty=1,
       col=c(par('fg'), 'blue', 'red', 'green', 'orange') )
```

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
Beta distribution becomes more right skewed with smaller α values or higher β values.
</p>
<br>

### 10.19
The output voltage for an electric circuit is specified to be 130. A sample of 40 independent
readings on the voltage for this circuit gave a sample mean 128.6 and standard deviation 2.1.
Test the hypothesis that the average output voltage is 130 against the alternative that it is less
than 130. Use a test with level .05. 

<br>

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;">  $H_0$ :μ = 130 against $H_a$ :μ < 130. </p>

\def\Ybar{\overline{Y}}

<p style="color: Navy;> 
\begin{align}
 Z = (\frac {\Ybar - μ_0}{σ/\sqrt{n}})
\end{align}
</p>

<p style="color: Navy;"> 
The rejection region, with α = .05, is given by {z < $z_{0.05}$ = -1.645}. The population variance σ2 is not known, but it can be estimated (because n = 40 is sufficiently large) by the sample standard deviation s = 2.1.
</p>

<p style="color: Navy;"> 
Thus, the observed value of the test statistic is approximately
</p>

<p style="color: Navy;"> 
\begin{align}
Z = (\frac {\Ybar - μ_0}{s/\sqrt{n}}) = (\frac {128.6-130}{2.1/\sqrt{40}}) = -4.216
\end{align}
</p>

<p style="color: Navy;"> 
Because the observed value of Z lies in the acceptance region (because z = -4.216 is less than
$z_{0.05}$ = -1.645), we reject $H_0$ :μ = 130. 
</p>
<br>

### 10.21
Shear strength measurements derived from unconfined compression tests for two types of soils
gave the results shown in the following table (measurements in tons per square foot). Do the
soils appear to differ with respect to average shear strength, at the 1% significance level?



|Soil Type I                              |  Soil Type II           |
|-----------------------------------------|-------------------------|
| n1                             = 30     |    n2           = 35    |
| $\bar{y}_1$                    = 1.65   |    $\bar{y}_2$  = 1.43  |
| s1                             = 0.26   |    s2           = 0.22  |
--------------------------------------------------------------------

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
Let μ1 and μ2 denote the true mean reaction times for Soil Type I and Soil Type II, respectively.
If we wish to test the hypothesis that the means differ, we must test $H_0$ : (μ1−μ2) = 0
against $H_a$ : (μ1 − μ2) = 0. The two-sided alternative permits us to detect either the
case μ1 > μ2 or the reverse case μ2 > μ1; in either case, $H_0$ is false.
</p>
<br>

<p style="color: Navy;"> 
The point estimator of (μ1−μ2) is ($\bar{Y}_1$ − $\bar{Y}_2$). Because the samples are independent and both are large, this estimator satisfies
the assumptions necessary to develop a large-sample test. Hence, if we desire to test $H_0$ :μ1 − μ2 = $D_0$ (where $D_0$ is some fixed value) versus any alternative, the test statistic is given by
</p>
<br>

<p style="color: Navy;"> 
\begin{align}
Z = \frac {(\Ybar_1 - \Ybar_2) - D_0}{\sqrt{\frac{σ^2_1}{n_1}+\frac{σ^2_1}{n_2}}}
\end{align}
</p>
<br>

<p style="color: Navy;"> 
For α = 0.01, we reject $H_0$ for |z| > $z_{α/2}$ = $z_{0.005}$ = 2.575.
</p>

<p style="color: Navy;"> 
\begin{align}
Z = \frac {(\Ybar_1 - \Ybar_2) - 0}{\sqrt{\frac{σ^2_1}{n_1}+\frac{σ^2_1}{n_2}}} = \frac {(1.65 - 1.43)}{\sqrt{\frac{0.0676}{30}+\frac{0.0484}{35}}} = 3.648
\end{align}
</p>
<br>

<p style="color: Navy;"> 
Because the observed value of Z lies in the rejection region (because z = 3.648 is larger than
|$z_{0.01}$| = 2.575), we reject $H_0$ :μ1 − μ2 = 0. 
</p>
<br>

### 11.31
Using a chemical procedure called differential pulse polarography, a chemist measured the
peak current generated (in microamperes, μA) when solutions containing different amounts of
nickel (measured in parts per billion, ppb) are added to different portions of the same buffer
Is there sufficient evidence to indicate that peak current increases as nickel concentrations
increase? Use α = 0.05.


|x = Ni (ppb)   |   y = Peak Current (μA) |
|---------------|-------------------------|
| 19.1          |        .095             |
| 38.2          |        .174             |
| 57.3          |        .256             |
| 76.2          |        .348             |
| 95            |        .429             |    
| 114           |        .500             |
| 131           |        .580             |
| 150           |        .651             |
| 170           |        .722             |
------------------------------------------

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;">  $H_0$ :$β_1$ = 0 against $H_a$ :$β_1$ $\neq$ 0. </p>


```{r echo=TRUE}
x <- c(19.1, 38.2, 57.3, 76.2, 95, 114, 131, 150, 170)
y <- c(.095, .174, .256, .348, .429, .500, .580, .651, .722)
summary(lm(y~x))
```

<p style="color: Navy;">
From the output, the fitted model is $\hat{y}$ = 0.01875 + 0.004215x. 
</p>

<p style="color: Navy;">
To test $H_0$: $β_1$ = 0 against
$H_a$: $β_1$ ≠ 0, because the p–value is quite small indicating a very significant test statistic.
Thus, $H_0$ is rejected and we can conclude that peak current increases as nickel
concentrations increase (p–value is 2.37e-11 divided by 2).
</p>
<br>

### 11.69
The manufacturer of Lexus automobiles has steadily increased sales since the 1989 launch of
that brand in the United States. However, the rate of increase changed in 1996 when Lexus
introduced a line of trucks. The sales of Lexus vehicles from 1996 to 2003 are shown in the
accompanying table.

|    x   |    y   |
|--------|--------|
|  1996  |  18.5  |
|  1997  |  22.6  |
|  1998  |  27.2  |
|  1999  |  31.2  |
|  2000  |  33.0  |
|  2001  |  44.9  |
|  2002  |  49.4  |
|  2003  |  35.0  |
-------------------

a- Letting Y denote sales and x denote the coded year (−7 for 1996, −5 for 1997, through 7
for 2003), fit the model Y = β0 + β1x + ε.

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

```{r echo=TRUE}
x <- c(-7, -5, -3, -1, 1, 3, 5, 7)
y <- c(18.5,22.6,27.2,31.2,33.0,44.9,49.4,35.0)
lm(y~x)
```
<p style="color: Navy;"> 
$\hat{y}$ = 32.725 + 1.812x
</p>
<br>

b- For the same data, fit the model Y = β0 + β1x + β2x2 + ε.

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

```{r echo=TRUE}
x <- c(-7, -5, -3, -1, 1, 3, 5, 7)
y <- c(18.5,22.6,27.2,31.2,33.0,44.9,49.4,35.0)
lm(y~x+I(x^2))
```
<p style="color: Navy;"> 
$\hat{y}$ = 35.5625 + 1.8119x - 0.1351$x^2$
</p>
<br>


# 2. Imputation techniques

### 1. 
Which type of missing mechanism do you prefer to get a good imputation?

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
Missingness completely at random because our predictions will not be baised by throwing out data, however the the variance will be larger due to reduced sample size.
</p>
<br>

### 2.
Say something about simple random imputation and regression imputation of a single variable.

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
Simple Random imputation imputes missing values with observed values of the same variable randomly and ignores all the information given by other predictors.
</p>
<br>

<p style="color: Navy;"> 
Regression Imputation can be deterministic or random: 
</p>

<p style="color: Navy;"> 
Deterministic regression imputation: predict the missing data by using simple linear regression model from the correlation between the predictor variables and the observed values of the same variable, however all the predicted values (of the missing values) will fall on the linear regression line, and the variance of the new data will be less than the original.
</p>
<br>

<p style="color: Navy;"> 
Random regression imputation: Adds the prediction error to the predicted values of the missing values, it better convey the range of predicted values.
</p>
<br>

### 3.
Explain shortly what Multiple Imputation is.

<p style="color: Navy; font-weight: bold"> 
Answer:
</p>

<p style="color: Navy;"> 
Multiple Imputation can be done through either fitting a multivariate model to all the variables that have missingness in what is called Routine multivariate imputation, it usually used as off-the-shelf model because it requires a lot of effort to set up.
</p>

<p style="color: Navy;"> 
Or Iterative regression imputation which involve multiple steps, if the variables with missingness are a matrix Y with columns $Y_1$, . . . , $Y_k$ and the fully observed predictors are X, this entails first imputing all the missing Y values using some
crude approach (for example, choosing imputed values for each variable by randomly selecting from the observed outcomes of that variable); and then imputing $Y_1$ given $Y_2$, . . . , $Y_k$ and X; imputing $Y_2$ given $Y_1$, $Y_3$, . . . , $Y_k$ and X (using the newly imputed values for $Y_1$), and so forth, randomly imputing each variable
and looping through until approximate convergence.
</p>

<p style="color: Navy;"> 
Iterative regression imputation is more manual and the imputer can tweek it as he see fit but require carefulness and consistency. 
</p>


